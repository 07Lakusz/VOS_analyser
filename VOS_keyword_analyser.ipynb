{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Automated Network Analysis Report Generator\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Welcome! This notebook automates the process of creating a detailed PDF report from your network analysis data (e.g., from VOSviewer).\n",
        "## Quick Start Guide\n",
        "Follow these three simple steps to generate your report.\n",
        "### Step 1: Run the Setup Cells\n",
        "Run the first few cells in order to install the necessary libraries and load the analysis functions. You do not need to make any changes to them.\n",
        "### Step 2: Run the Main Workflow Cell\n",
        "Execute the final cell labeled \"EXECUTION\". This will start the interactive process:\n",
        "Upload Your Files: A file upload dialog will pop up. Please select all your data files at once:\n",
        "Your Node file (must end with _node.csv)\n",
        "Your Edge file (must end with _edge.csv)\n",
        "Any PNG images of your network visualizations.\n",
        "Start the Analysis: After the files finish uploading, a green Start Analysis button will appear. Click it to begin processing your data.\n",
        "Download Your Report: Once the analysis is complete, a blue Download Report button will appear. Click it to save the final PDF to your computer.\n",
        "### Step 3: Clean Up\n",
        "After you have downloaded your report, a red Clean Up Files button will become active. Click this button to securely delete all your data and the generated report from this temporary Colab session.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**That's it! If you encounter any errors, the notebook will automatically clean up the files and provide an error message. Just re-run the final \"EXECUTION\" cell to try again.**"
      ],
      "metadata": {
        "id": "Ld9H3LcBsVaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================================\n",
        "# CELL 1: SETUP\n",
        "#\n",
        "# This cell installs and imports all necessary libraries for the analysis.\n",
        "# It should be run once at the beginning of the session.\n",
        "#\n",
        "#===================================================================\n",
        "!pip install reportlab -q\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib import colors\n",
        "from reportlab.lib.units import cm\n",
        "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak, Image\n",
        "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, HTML\n",
        "import os\n",
        "import io\n",
        "\n",
        "print(\"Setup complete. You can now run the other cells.\")"
      ],
      "metadata": {
        "id": "FmeaJBn3TfNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================================\n",
        "# CELL 2: CONFIGURATION\n",
        "#\n",
        "# This cell contains a centralized configuration dictionary.\n",
        "# All user-adjustable parameters, such as filenames, report titles, and\n",
        "# column headers from the input data, should be set here.\n",
        "#\n",
        "#===================================================================\n",
        "\n",
        "# This dictionary holds all configurable settings.\n",
        "config = {\n",
        "    # --- Report Settings ---\n",
        "    \"PDF_FILENAME\": \"Network_Analysis_Report.pdf\",\n",
        "    \"REPORT_TITLE\": \"Network Analysis Report\",\n",
        "    \"TOP_N_KEYWORDS\": 10,       # Number of top keywords to show in tables\n",
        "    \"TOP_N_INFLUENTIAL\": 5,     # Number of influential keywords for in-depth analysis\n",
        "\n",
        "    # --- Data Column Names ---\n",
        "    \"NODE_ID_COL\": 'id',\n",
        "    \"NODE_LABEL_COL\": 'label',\n",
        "    \"NODE_CLUSTER_COL\": 'cluster',\n",
        "    \"NODE_OCCURRENCES_COL\": 'weight<Occurrences>',\n",
        "    \"NODE_LINK_STRENGTH_COL\": 'weight<Total link strength>',\n",
        "    \"NODE_AVG_PUB_YEAR_COL\": 'score<Avg. pub. year>',\n",
        "    \"EDGE_SOURCE_COL\": 'source',\n",
        "    \"EDGE_TARGET_COL\": 'target',\n",
        "    \"EDGE_WEIGHT_COL\": 'weight',\n",
        "}\n",
        "\n",
        "print(\"Configuration loaded.\")"
      ],
      "metadata": {
        "id": "9Lrvvjm9S6ZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================================\n",
        "# CELL 3: DATA LOADING & ANALYSIS FUNCTIONS\n",
        "#\n",
        "# This cell defines all functions responsible for core data processing and\n",
        "# network analysis. Each function adheres to the Single Responsibility\n",
        "# Principle, focusing on one specific task (e.g., loading data, calculating\n",
        "# centrality, analyzing clusters).\n",
        "#\n",
        "#===================================================================\n",
        "\n",
        "def load_data(uploaded_files, config):\n",
        "    \"\"\"Loads, validates, and prepares node, edge, and graph data.\n",
        "\n",
        "    Identifies node and edge CSV files from the uploaded dictionary, reads\n",
        "    them into pandas DataFrames, validates required columns, and constructs\n",
        "    a NetworkX graph object.\n",
        "\n",
        "    Args:\n",
        "        uploaded_files (dict): A dictionary where keys are filenames and\n",
        "            values are file content in bytes, as provided by google.colab.\n",
        "        config (dict): The configuration dictionary with column names.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - pd.DataFrame: The edges DataFrame.\n",
        "            - pd.DataFrame: The nodes DataFrame, indexed by node ID.\n",
        "            - nx.Graph: The constructed network graph.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the required '_edge.csv' or '_node.csv'\n",
        "            files are not found in the uploaded files.\n",
        "        ValueError: If the node file is missing required columns.\n",
        "    \"\"\"\n",
        "    edge_file_name = next((name for name in uploaded_files if '_edge.csv' in name), None)\n",
        "    node_file_name = next((name for name in uploaded_files if '_node.csv' in name), None)\n",
        "\n",
        "    if not edge_file_name or not node_file_name:\n",
        "        raise FileNotFoundError(\"Could not find the required '_edge.csv' or '_node.csv' files.\")\n",
        "\n",
        "    edges_df = pd.read_csv(io.BytesIO(uploaded_files[edge_file_name]), sep=\",\", header=None,\n",
        "                           names=[config['EDGE_SOURCE_COL'], config['EDGE_TARGET_COL'], config['EDGE_WEIGHT_COL']])\n",
        "    nodes_df = pd.read_csv(io.BytesIO(uploaded_files[node_file_name]), sep=\",\")\n",
        "\n",
        "    # --- Data Validation ---\n",
        "    required_node_cols = {\n",
        "        config['NODE_ID_COL'], config['NODE_LABEL_COL'], config['NODE_CLUSTER_COL'],\n",
        "        config['NODE_OCCURRENCES_COL'], config['NODE_LINK_STRENGTH_COL'], config['NODE_AVG_PUB_YEAR_COL']\n",
        "    }\n",
        "    if not required_node_cols.issubset(nodes_df.columns):\n",
        "        missing = required_node_cols - set(nodes_df.columns)\n",
        "        raise ValueError(f\"Node file is missing required columns: {missing}.\")\n",
        "\n",
        "    nodes_df = nodes_df.set_index(config['NODE_ID_COL'])\n",
        "    graph = nx.from_pandas_edgelist(edges_df, config['EDGE_SOURCE_COL'], config['EDGE_TARGET_COL'], config['EDGE_WEIGHT_COL'])\n",
        "    return edges_df, nodes_df, graph\n",
        "\n",
        "def analyze_occurrences(nodes_df, config):\n",
        "    \"\"\"Analyzes keywords by their occurrence count.\n",
        "\n",
        "    Args:\n",
        "        nodes_df (pd.DataFrame): The DataFrame containing node data.\n",
        "        config (dict): The configuration dictionary with settings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of top keywords by occurrence.\n",
        "    \"\"\"\n",
        "    df = nodes_df.nlargest(config['TOP_N_KEYWORDS'], config['NODE_OCCURRENCES_COL'])\n",
        "    df = df[[config['NODE_LABEL_COL'], config['NODE_OCCURRENCES_COL']]]\n",
        "    return df.rename(columns={config['NODE_OCCURRENCES_COL']: 'Occurrences', config['NODE_LABEL_COL']: 'Keyword'})\n",
        "\n",
        "def analyze_link_strength(nodes_df, config):\n",
        "    \"\"\"Analyzes keywords by their total link strength.\n",
        "\n",
        "    Args:\n",
        "        nodes_df (pd.DataFrame): The DataFrame containing node data.\n",
        "        config (dict): The configuration dictionary with settings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of top keywords by link strength.\n",
        "    \"\"\"\n",
        "    df = nodes_df.nlargest(config['TOP_N_KEYWORDS'], config['NODE_LINK_STRENGTH_COL'])\n",
        "    df = df[[config['NODE_LABEL_COL'], config['NODE_LINK_STRENGTH_COL']]]\n",
        "    return df.rename(columns={config['NODE_LINK_STRENGTH_COL']: 'Total Link Strength', config['NODE_LABEL_COL']: 'Keyword'})\n",
        "\n",
        "def analyze_centrality(graph, nodes_df, config):\n",
        "    \"\"\"Calculates Degree, Betweenness, Closeness, and Eigenvector centralities.\n",
        "\n",
        "    Args:\n",
        "        graph (nx.Graph): The NetworkX graph object.\n",
        "        nodes_df (pd.DataFrame): The DataFrame containing node data.\n",
        "        config (dict): The configuration dictionary with settings.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of DataFrames, with each key representing a\n",
        "              centrality metric ('degree', 'betweenness', etc.).\n",
        "    \"\"\"\n",
        "    degree = pd.DataFrame(nx.degree_centrality(graph).items(), columns=[config['NODE_ID_COL'], 'Degree Centrality']).set_index(config['NODE_ID_COL'])\n",
        "    betweenness = pd.DataFrame(nx.betweenness_centrality(graph).items(), columns=[config['NODE_ID_COL'], 'Betweenness Centrality']).set_index(config['NODE_ID_COL'])\n",
        "    closeness = pd.DataFrame(nx.closeness_centrality(graph).items(), columns=[config['NODE_ID_COL'], 'Closeness Centrality']).set_index(config['NODE_ID_COL'])\n",
        "\n",
        "    try:\n",
        "        eigenvector = pd.DataFrame(nx.eigenvector_centrality(graph, max_iter=1000, weight=config['EDGE_WEIGHT_COL']).items(),\n",
        "                                   columns=[config['NODE_ID_COL'], 'Eigenvector Centrality']).set_index(config['NODE_ID_COL'])\n",
        "    except nx.PowerIterationFailedConvergence:\n",
        "        print(\"Warning: Eigenvector centrality did not converge. This metric will be skipped.\")\n",
        "        eigenvector = pd.DataFrame(columns=[config['NODE_ID_COL'], 'Eigenvector Centrality']).set_index(config['NODE_ID_COL'])\n",
        "\n",
        "    centrality_df = nodes_df.join([degree, betweenness, closeness, eigenvector])\n",
        "\n",
        "    results = {\n",
        "        'degree': centrality_df.nlargest(config['TOP_N_KEYWORDS'], 'Degree Centrality')[[config['NODE_LABEL_COL'], 'Degree Centrality']].rename(columns={config['NODE_LABEL_COL']: 'Keyword'}),\n",
        "        'betweenness': centrality_df.nlargest(config['TOP_N_KEYWORDS'], 'Betweenness Centrality')[[config['NODE_LABEL_COL'], 'Betweenness Centrality']].rename(columns={config['NODE_LABEL_COL']: 'Keyword'}),\n",
        "        'closeness': centrality_df.nlargest(config['TOP_N_KEYWORDS'], 'Closeness Centrality')[[config['NODE_LABEL_COL'], 'Closeness Centrality']].rename(columns={config['NODE_LABEL_COL']: 'Keyword'}),\n",
        "        'eigenvector': centrality_df.nlargest(config['TOP_N_KEYWORDS'], 'Eigenvector Centrality')[[config['NODE_LABEL_COL'], 'Eigenvector Centrality']].rename(columns={config['NODE_LABEL_COL']: 'Keyword'})\n",
        "    }\n",
        "    return results\n",
        "\n",
        "def analyze_clusters(nodes_df, graph, config):\n",
        "    \"\"\"Analyzes thematic clusters by calculating coherence and top labels.\n",
        "\n",
        "    Args:\n",
        "        nodes_df (pd.DataFrame): The DataFrame containing node data.\n",
        "        graph (nx.Graph): The NetworkX graph object.\n",
        "        config (dict): The configuration dictionary with settings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame containing cluster ID, coherence score,\n",
        "                      and top labels for each cluster.\n",
        "    \"\"\"\n",
        "    cluster_labels = nodes_df.groupby(config['NODE_CLUSTER_COL'])[config['NODE_LABEL_COL']].agg(\n",
        "        lambda x: ', '.join(sorted(x, key=lambda y: -len(y))[:5]) + ('...' if len(x) > 5 else '')).to_dict()\n",
        "\n",
        "    coherence_scores = {}\n",
        "    for cluster_id in nodes_df[config['NODE_CLUSTER_COL']].unique():\n",
        "        cluster_nodes = nodes_df[nodes_df[config['NODE_CLUSTER_COL']] == cluster_id].index\n",
        "        subgraph = graph.subgraph(cluster_nodes)\n",
        "        if subgraph.number_of_edges() > 0:\n",
        "            avg_weight = subgraph.size(weight=config['EDGE_WEIGHT_COL']) / subgraph.number_of_edges()\n",
        "            coherence_scores[cluster_id] = avg_weight\n",
        "        else:\n",
        "            coherence_scores[cluster_id] = 0\n",
        "\n",
        "    coherence_df = pd.DataFrame(coherence_scores.items(), columns=['Cluster ID', 'Average Edge Weight']).round(4)\n",
        "    coherence_df['Top 5 Cluster Labels'] = coherence_df['Cluster ID'].map(cluster_labels)\n",
        "    return coherence_df\n",
        "\n",
        "def analyze_temporal_trends(nodes_df, config):\n",
        "    \"\"\"Identifies the most recent keywords based on average publication year.\n",
        "\n",
        "    Args:\n",
        "        nodes_df (pd.DataFrame): The DataFrame containing node data.\n",
        "        config (dict): The configuration dictionary with settings.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame of top keywords from recent publications.\n",
        "    \"\"\"\n",
        "    df = nodes_df.nlargest(config['TOP_N_KEYWORDS'], config['NODE_AVG_PUB_YEAR_COL'])\n",
        "    df = df[[config['NODE_LABEL_COL'], config['NODE_AVG_PUB_YEAR_COL']]]\n",
        "    df = df.rename(columns={config['NODE_AVG_PUB_YEAR_COL']: 'Avg. Pub. Year'})\n",
        "    return df.rename(columns={config['NODE_LABEL_COL']: 'Keyword Label'})[['Keyword Label', 'Avg. Pub. Year']]\n",
        "\n",
        "def analyze_influential_keywords(nodes_df, edges_df, config):\n",
        "    \"\"\"Identifies top influential keywords and their strongest connections.\n",
        "\n",
        "    Args:\n",
        "        nodes_df (pd.DataFrame): The DataFrame containing node data.\n",
        "        edges_df (pd.DataFrame): The DataFrame containing edge data.\n",
        "        config (dict): The configuration dictionary with settings.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries, where each dictionary contains an\n",
        "              influential keyword and a DataFrame of its top connections.\n",
        "    \"\"\"\n",
        "    influential_nodes = nodes_df.nlargest(config['TOP_N_INFLUENTIAL'], config['NODE_LINK_STRENGTH_COL'])\n",
        "    connections_data = []\n",
        "\n",
        "    for node_id, row in influential_nodes.iterrows():\n",
        "        keyword_label = row[config['NODE_LABEL_COL']]\n",
        "        connected_edges = edges_df[(edges_df[config['EDGE_SOURCE_COL']] == node_id) | (edges_df[config['EDGE_TARGET_COL']] == node_id)]\n",
        "\n",
        "        other_nodes = connected_edges.apply(lambda r: r[config['EDGE_TARGET_COL']] if r[config['EDGE_SOURCE_COL']] == node_id else r[config['EDGE_SOURCE_COL']], axis=1)\n",
        "        connection_weights = pd.DataFrame({'other_node_id': other_nodes, 'weight': connected_edges[config['EDGE_WEIGHT_COL']]})\n",
        "\n",
        "        top_connections = connection_weights.groupby('other_node_id')['weight'].sum().nlargest(config['TOP_N_KEYWORDS']).reset_index()\n",
        "        top_connections['Connected Keyword'] = top_connections['other_node_id'].map(nodes_df[config['NODE_LABEL_COL']])\n",
        "\n",
        "        connections_data.append({\n",
        "            'Keyword': keyword_label,\n",
        "            'Connections': top_connections[['Connected Keyword', 'weight']].rename(columns={'weight': 'Connection Strength'})\n",
        "        })\n",
        "    return connections_data\n",
        "\n",
        "print(\"Analysis functions defined.\")"
      ],
      "metadata": {
        "id": "axru4pHrTmyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================================\n",
        "# CELL 4: PDF REPORT GENERATION FUNCTIONS\n",
        "#\n",
        "# This cell contains all helper functions related to building the PDF report\n",
        "# using the ReportLab library. This includes functions for creating tables,\n",
        "# adding headers/footers, and structuring report sections.\n",
        "#\n",
        "#===================================================================\n",
        "\n",
        "def dataframe_to_table(df):\n",
        "    \"\"\"Converts a Pandas DataFrame to a styled ReportLab Table object.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to convert.\n",
        "\n",
        "    Returns:\n",
        "        reportlab.platypus.Table: The styled table object.\n",
        "    \"\"\"\n",
        "    styles = getSampleStyleSheet()\n",
        "    cell_style = ParagraphStyle('CellStyle', parent=styles['Normal'], fontSize=9, leading=10)\n",
        "\n",
        "    header_style = ParagraphStyle('HeaderStyle', parent=cell_style, fontName='Helvetica-Bold', fontSize=10, textColor=colors.white)\n",
        "    data = [[Paragraph(col, header_style) for col in df.columns]]\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        formatted_row = []\n",
        "        for cell in row:\n",
        "            text = f\"{cell:.4f}\" if isinstance(cell, float) else str(cell)\n",
        "            formatted_row.append(Paragraph(text, cell_style))\n",
        "        data.append(formatted_row)\n",
        "\n",
        "    table = Table(data, repeatRows=1)\n",
        "    table.setStyle(TableStyle([\n",
        "        ('BACKGROUND', (0,0), (-1,0), colors.HexColor('#4F81BD')),\n",
        "        ('ALIGN', (0,0), (-1,-1), 'LEFT'),\n",
        "        ('VALIGN', (0,0), (-1,-1), 'TOP'),\n",
        "        ('GRID', (0,0), (-1,-1), 0.5, colors.HexColor('#B8CCE4')),\n",
        "        ('BACKGROUND', (0,1), (-1,-1), colors.HexColor('#DCE6F1')),\n",
        "    ]))\n",
        "    return table\n",
        "\n",
        "def create_report_section(elements, styles, title, image_path, image_caption, tables):\n",
        "    \"\"\"Creates a standard section for the report with title, image, and tables.\n",
        "\n",
        "    Args:\n",
        "        elements (list): The list of ReportLab flowables to append to.\n",
        "        styles (dict): The stylesheet dictionary from ReportLab.\n",
        "        title (str): The main heading for this section.\n",
        "        image_path (str): The file path to the visualization image.\n",
        "        image_caption (str): The caption to display below the image.\n",
        "        tables (list): A list of tuples, where each tuple contains\n",
        "                       (table_title, table_caption, pd.DataFrame).\n",
        "    \"\"\"\n",
        "    elements.append(Paragraph(title, styles['Heading1']))\n",
        "    if image_path and os.path.exists(image_path):\n",
        "        img_title = os.path.basename(image_path).replace('.png','').replace('_', ' ').title()\n",
        "        elements.append(Paragraph(f\"{img_title} Visualization\", styles['Heading2']))\n",
        "        elements.append(Paragraph(image_caption, styles['BodyText']))\n",
        "        elements.append(Image(image_path, width=15*cm, height=12*cm))\n",
        "        elements.append(Spacer(1, 0.5 * cm))\n",
        "    else:\n",
        "        print(f\"Warning: Visualization for '{title}' not found. Skipping image.\")\n",
        "\n",
        "    for tbl_title, tbl_caption, tbl_df in tables:\n",
        "        if not tbl_df.empty:\n",
        "            elements.append(Paragraph(tbl_title, styles['Heading3']))\n",
        "            elements.append(Paragraph(tbl_caption, styles['BodyText']))\n",
        "            elements.append(dataframe_to_table(tbl_df))\n",
        "            elements.append(Spacer(1, 0.5*cm))\n",
        "    elements.append(PageBreak())\n",
        "\n",
        "def add_header_footer(canvas, doc, config):\n",
        "    \"\"\"Adds a header and page number to each page of the PDF.\n",
        "\n",
        "    Args:\n",
        "        canvas (reportlab.pdfgen.canvas.Canvas): The canvas object.\n",
        "        doc (reportlab.platypus.SimpleDocTemplate): The document object.\n",
        "        config (dict): The configuration dictionary with report settings.\n",
        "    \"\"\"\n",
        "    canvas.saveState()\n",
        "    # Header\n",
        "    header_style = ParagraphStyle(name='Header', fontName='Helvetica-Bold', fontSize=14, alignment=0, textColor=colors.darkblue)\n",
        "    header = Paragraph(config['REPORT_TITLE'], header_style)\n",
        "    header.wrapOn(canvas, doc.width, doc.topMargin)\n",
        "    header.drawOn(canvas, doc.leftMargin, doc.pagesize[1] - 1.5*cm)\n",
        "    # Footer (Page Number)\n",
        "    canvas.setFont('Helvetica', 9)\n",
        "    canvas.drawRightString(doc.pagesize[0] - doc.rightMargin, 1.5*cm, f\"Page {doc.page}\")\n",
        "    canvas.restoreState()\n",
        "\n",
        "def find_image_paths(uploaded_files):\n",
        "    \"\"\"Finds the paths for the standard visualization images.\n",
        "\n",
        "    Args:\n",
        "        uploaded_files (dict): The dictionary of uploaded files.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping image types ('network', 'density', 'overlay')\n",
        "              to their filenames.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"network\": next((name for name in uploaded_files if \"network\" in name and name.endswith('.png')), None),\n",
        "        \"density\": next((name for name in uploaded_files if \"density\" in name and name.endswith('.png')), None),\n",
        "        \"overlay\": next((name for name in uploaded_files if \"overlay\" in name and name.endswith('.png')), None)\n",
        "    }\n",
        "\n",
        "print(\"Report generation functions defined.\")"
      ],
      "metadata": {
        "id": "ojj2pSevT9KN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================================\n",
        "# CELL 5: MAIN ORCHESTRATOR\n",
        "#\n",
        "# This cell contains the main function that coordinates the entire workflow,\n",
        "# from loading data to performing all analyses and finally assembling the\n",
        "# PDF report.\n",
        "#\n",
        "#===================================================================\n",
        "\n",
        "def run_analysis_and_generate_report(uploaded_files, config):\n",
        "    \"\"\"Orchestrates the analysis and report generation process.\n",
        "\n",
        "    This function calls the necessary analysis functions in order,\n",
        "    assembles the results into a structured report, and builds the PDF.\n",
        "\n",
        "    Args:\n",
        "        uploaded_files (dict): The dictionary of uploaded files from Colab.\n",
        "        config (dict): The global configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the report was generated successfully, False otherwise.\n",
        "    \"\"\"\n",
        "    doc = SimpleDocTemplate(config['PDF_FILENAME'], pagesize=A4, leftMargin=2*cm, rightMargin=2*cm, topMargin=2.5*cm, bottomMargin=2.5*cm)\n",
        "    styles = getSampleStyleSheet()\n",
        "    elements = []\n",
        "\n",
        "    try:\n",
        "        # --- 1. Load Data and Find Images ---\n",
        "        print(\"Step 1/5: Loading and validating data...\")\n",
        "        edges_df, nodes_df, graph = load_data(uploaded_files, config)\n",
        "        image_paths = find_image_paths(uploaded_files)\n",
        "        print(\"Data loaded successfully.\")\n",
        "\n",
        "        # --- 2. Perform All Analyses (SRP & DRY) ---\n",
        "        # Each analysis is run once and its results are stored.\n",
        "        print(\"Step 2/5: Performing all analyses...\")\n",
        "        occurrences_df = analyze_occurrences(nodes_df, config)\n",
        "        link_strength_df = analyze_link_strength(nodes_df, config)\n",
        "        centrality_results = analyze_centrality(graph, nodes_df, config)\n",
        "        cluster_coherence_df = analyze_clusters(nodes_df, graph, config)\n",
        "        temporal_keywords_df = analyze_temporal_trends(nodes_df, config)\n",
        "        influential_connections = analyze_influential_keywords(nodes_df, edges_df, config)\n",
        "        print(\"All analyses complete.\")\n",
        "\n",
        "        # --- 3. Build Report Sections ---\n",
        "        print(\"Step 3/5: Building report sections...\")\n",
        "\n",
        "        # Section A: Overall Network Analysis\n",
        "        overall_tables = [\n",
        "            (f\"Top {config['TOP_N_KEYWORDS']} Keywords by Occurrence\", \"Highest raw count in source documents.\", occurrences_df),\n",
        "            (f\"Top {config['TOP_N_KEYWORDS']} Keywords by Link Strength\", \"Strongest cumulative co-occurrence links.\", link_strength_df),\n",
        "            (f\"Top {config['TOP_N_KEYWORDS']} Keywords by Degree Centrality\", \"Most directly connected keywords.\", centrality_results['degree']),\n",
        "            (f\"Top {config['TOP_N_KEYWORDS']} Keywords by Betweenness Centrality\", \"'Bridge' keywords connecting different areas.\", centrality_results['betweenness']),\n",
        "            (f\"Top {config['TOP_N_KEYWORDS']} Keywords by Closeness Centrality\", \"Keywords that can reach all others most quickly.\", centrality_results['closeness']),\n",
        "            (f\"Top {config['TOP_N_KEYWORDS']} Keywords by Eigenvector Centrality\", \"Influential keywords connected to other influential ones.\", centrality_results['eigenvector'])\n",
        "        ]\n",
        "        create_report_section(elements, styles, \"Overall Network Analysis\", image_paths[\"network\"],\n",
        "                              \"This visualization depicts the overall co-occurrence network of keywords.\", overall_tables)\n",
        "\n",
        "        # Section B: Thematic Analysis\n",
        "        thematic_tables = [(\"Cluster Coherence and Top Labels\", \"Shows cluster coherence and representative keywords.\", cluster_coherence_df)]\n",
        "        create_report_section(elements, styles, \"Thematic Analysis (Clusters)\", image_paths[\"density\"],\n",
        "                              \"This visualization highlights keyword density, indicating thematic clusters.\", thematic_tables)\n",
        "\n",
        "        # Section C: Temporal Analysis\n",
        "        temporal_tables = [(\"Most Recent Keywords\", \"Lists keywords from recent publications, indicating emerging trends.\", temporal_keywords_df)]\n",
        "        create_report_section(elements, styles, \"Temporal Analysis\", image_paths[\"overlay\"],\n",
        "                              \"This visualization shows the temporal evolution of keywords.\", temporal_tables)\n",
        "\n",
        "        # Section D: Keyword-Level Analysis\n",
        "        elements.append(Paragraph(\"Keyword-Level Analysis\", styles['Heading1']))\n",
        "        for connection in influential_connections:\n",
        "            elements.append(Paragraph(f\"Influential Keyword: {connection['Keyword']}\", styles['Heading2']))\n",
        "            elements.append(Paragraph(f\"Top co-occurrence connections for '{connection['Keyword']}'.\", styles['BodyText']))\n",
        "            elements.append(dataframe_to_table(connection['Connections']))\n",
        "            elements.append(Spacer(1, 0.5 * cm))\n",
        "        print(\"Report sections constructed.\")\n",
        "\n",
        "        # --- 4. Generate PDF ---\n",
        "        print(\"\\nStep 4/5: Generating PDF document... this may take a moment.\")\n",
        "        header_footer_with_config = lambda c, d: add_header_footer(c, d, config)\n",
        "        doc.build(elements, onFirstPage=header_footer_with_config, onLaterPages=header_footer_with_config)\n",
        "\n",
        "        print(f\"\\nReport '{config['PDF_FILENAME']}' generated successfully!\")\n",
        "        return True\n",
        "\n",
        "    except (FileNotFoundError, ValueError, KeyError) as e:\n",
        "        print(f\"\\nERROR: {e}\")\n",
        "        print(\"   Please check your uploaded files and the settings in the 'Configuration' cell, then try again.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"Main orchestrator function defined.\")"
      ],
      "metadata": {
        "id": "9UEtXAjZUG2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===================================================================\n",
        "# CELL 6: EXECUTION\n",
        "#\n",
        "# This cell provides a simple, controlled workflow for the entire\n",
        "# analysis process.\n",
        "#\n",
        "#===================================================================\n",
        "\"\"\"\n",
        "Workflow:\n",
        "1.  Uses the standard `google.colab.files.upload()` pop-up for file selection.\n",
        "2.  Validates that the required files have been uploaded. If validation fails,\n",
        "    it automatically cleans up the uploaded files and instructs the user.\n",
        "3.  If validation succeeds, it presents a \"Start Analysis\" button.\n",
        "4.  If the analysis process encounters an error, it automatically cleans up\n",
        "    the uploaded files and reports the specific error.\n",
        "5.  If the analysis succeeds, it presents \"Download Report\" and \"Clean Up Files\"\n",
        "    buttons, where the cleanup action is only enabled after the download is\n",
        "    initiated.\n",
        "\"\"\"\n",
        "\n",
        "def _perform_cleanup(uploaded_files_dict, pdf_filename=None, output_widget=None):\n",
        "    \"\"\"\n",
        "    A central function to delete uploaded files and the generated PDF.\n",
        "\n",
        "    This helper function is used for both automatic cleanup on error and\n",
        "    manual cleanup initiated by the user.\n",
        "\n",
        "    Args:\n",
        "        uploaded_files_dict (dict): The dictionary of uploaded files from Colab.\n",
        "        pdf_filename (str, optional): The name of the generated PDF file to delete.\n",
        "        output_widget (ipywidgets.Output, optional): The widget to print\n",
        "            status messages to. If provided, the initial message is adjusted\n",
        "            for manual cleanup.\n",
        "    \"\"\"\n",
        "    message = \"Initiating automatic cleanup due to error...\"\n",
        "    if output_widget:\n",
        "        # This branch is for manual cleanup, so the message is different.\n",
        "        output_widget.clear_output()\n",
        "        message = \"Initiating cleanup...\"\n",
        "\n",
        "    print(message)\n",
        "\n",
        "    files_to_delete = list(uploaded_files_dict.keys())\n",
        "    if pdf_filename and os.path.exists(pdf_filename):\n",
        "        files_to_delete.append(pdf_filename)\n",
        "\n",
        "    if not files_to_delete:\n",
        "        print(\"No files to clean up.\")\n",
        "        return\n",
        "\n",
        "    deleted_count = 0\n",
        "    for filename in set(files_to_delete):\n",
        "        if os.path.exists(filename):\n",
        "            try:\n",
        "                os.remove(filename)\n",
        "                print(f\"  - Removed: {filename}\")\n",
        "                deleted_count += 1\n",
        "            except OSError as e:\n",
        "                print(f\"  - Error removing file {filename}: {e}\")\n",
        "\n",
        "    print(f\"\\nCleanup finished. Removed {deleted_count} file(s).\")\n",
        "\n",
        "\n",
        "# --- Step 1: Upload Files ---\n",
        "print(\"Please select all required and properly formatted files ('*_node.csv', '*_edge.csv', 'network.png', 'density.png', 'overlay.png') in one step. The upload process will ignore any other file uploaded.\")\n",
        "print(\"Please upload your data files. You will need:\")\n",
        "print(\"  1. A mandantory node file (e.g., '*my_data*_node.csv')\")\n",
        "print(\"  2. A mandantory edge file (e.g., '*my_data*_edge.csv')\")\n",
        "print(\"  3. Optionally network visualization images (as 'network.png', 'density.png', 'overlay.png')\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# --- Step 2: Validate Files and Present Next Action ---\n",
        "if not uploaded:\n",
        "    print(\"\\nError: No files were uploaded. Please run the cell again to select your files.\")\n",
        "else:\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Success: Uploaded {len(uploaded)} file(s):\")\n",
        "    for fn in uploaded.keys():\n",
        "        print(f\"  - {fn}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # Validate that the required files are present\n",
        "    filenames = uploaded.keys()\n",
        "    has_node = any('_node.csv' in f for f in filenames)\n",
        "    has_edge = any('_edge.csv' in f for f in filenames)\n",
        "\n",
        "    if has_node and has_edge:\n",
        "        # --- IF VALIDATION PASSES: Present the 'Start Analysis' Button ---\n",
        "        start_button = widgets.Button(description=\"Start Analysis\", button_style='success', icon='cogs')\n",
        "        analysis_output = widgets.Output()\n",
        "\n",
        "        def on_start_analysis_clicked(b):\n",
        "            \"\"\"\n",
        "            Handles the click event for the 'Start Analysis' button.\n",
        "\n",
        "            This function runs the main analysis process and, upon success,\n",
        "            displays the final download and cleanup actions. It handles\n",
        "            exceptions by initiating an automatic cleanup.\n",
        "\n",
        "            Args:\n",
        "                b (ipywidgets.Button): The button instance that was clicked.\n",
        "            \"\"\"\n",
        "            b.disabled = True\n",
        "            b.description = \"Analysis in Progress...\"\n",
        "\n",
        "            with analysis_output:\n",
        "                analysis_output.clear_output()\n",
        "                print(\"Starting analysis... This may take a moment.\")\n",
        "                try:\n",
        "                    success = run_analysis_and_generate_report(uploaded, config)\n",
        "                    if success:\n",
        "                        print(\"\\nSuccess: Analysis Complete. Your report is ready.\")\n",
        "                        # Define the final widgets\n",
        "                        download_button = widgets.Button(description=\"Download Report\", button_style='primary', icon='download')\n",
        "                        cleanup_button = widgets.Button(description=\"Clean Up Files\", button_style='danger', icon='trash', disabled=True)\n",
        "                        final_output = widgets.Output()\n",
        "\n",
        "                        def on_download_clicked(btn):\n",
        "                            \"\"\"\n",
        "                            Handles the download button click event.\n",
        "\n",
        "                            Initiates the file download and enables the cleanup button.\n",
        "\n",
        "                            Args:\n",
        "                                btn (ipywidgets.Button): The button instance.\n",
        "                            \"\"\"\n",
        "                            files.download(config['PDF_FILENAME'])\n",
        "                            btn.description = \"Download Started\"\n",
        "                            btn.disabled = True\n",
        "                            cleanup_button.disabled = False  # Enable cleanup\n",
        "\n",
        "                        def on_cleanup_clicked(btn):\n",
        "                            \"\"\"\n",
        "                            Handles the cleanup button click event.\n",
        "\n",
        "                            Calls the helper function to remove all session files.\n",
        "\n",
        "                            Args:\n",
        "                                btn (ipywidgets.Button): The button instance.\n",
        "                            \"\"\"\n",
        "                            with final_output:\n",
        "                                _perform_cleanup(uploaded, config['PDF_FILENAME'], output_widget=final_output)\n",
        "                                btn.description = \"Cleanup Complete\"\n",
        "                                btn.disabled = True\n",
        "                                download_button.disabled = True\n",
        "\n",
        "                        download_button.on_click(on_download_clicked)\n",
        "                        cleanup_button.on_click(on_cleanup_clicked)\n",
        "\n",
        "                        # Group buttons in a container for reliable display\n",
        "                        button_container = widgets.HBox([download_button, cleanup_button])\n",
        "\n",
        "                        display(HTML(\"<h3>Final Actions</h3><p>Please download your report, then clean up the files.</p>\"))\n",
        "                        display(button_container)\n",
        "                        display(final_output)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"\\nError: An unexpected error occurred during analysis: {e}\")\n",
        "                    _perform_cleanup(uploaded)\n",
        "\n",
        "        start_button.on_click(on_start_analysis_clicked)\n",
        "        display(HTML(\"<h3>Ready to Proceed</h3><p>Required files are present. Click the button below to start the analysis.</p>\"))\n",
        "        display(start_button, analysis_output)\n",
        "\n",
        "    else:\n",
        "        # --- IF VALIDATION FAILS: Show error and automatically clean up ---\n",
        "        print(\"\\nError: VALIDATION FAILED. Required files are missing.\")\n",
        "        print(\"   Please make sure both a '_node.csv' file and an '_edge.csv' file were selected.\")\n",
        "        _perform_cleanup(uploaded)"
      ],
      "metadata": {
        "id": "kLQ_NOJuUjk9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}